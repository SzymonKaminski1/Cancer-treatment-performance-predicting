{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_full = pd.read_csv('data_test.csv')\n",
    "X_train_full = pd.read_csv('data_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping observation with incomplete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test_full.dropna() \n",
    "X_train = X_train_full.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperating data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_train.Y\n",
    "X_train.drop(['Y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invastigating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variables:  17737 \n",
      "Number of observations:  644\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 644 entries, 0 to 643\n",
      "Columns: 17737 entries, ENSG00000000003 to ENSG00000266753\n",
      "dtypes: float64(17737)\n",
      "memory usage: 87.1 MB\n"
     ]
    }
   ],
   "source": [
    "print( \"Number of variables: \", len(X_train.columns), \"\\nNumber of observations: \", len(X_train.index))\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data is numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing coefficient of variation for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_var = {}\n",
    "var = X_train.var()\n",
    "mean = X_train.mean()\n",
    "for i in range(0, len(X_train.columns)):\n",
    "    coef_var[i] = var[i]/mean[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecing 500 columns with highest value of coefficient of variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_coef_var=sorted(coef_var, key=coef_var.get, reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coputing correlation of top 500 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000129824</th>\n",
       "      <th>ENSG00000184292</th>\n",
       "      <th>ENSG00000163993</th>\n",
       "      <th>ENSG00000122862</th>\n",
       "      <th>ENSG00000171345</th>\n",
       "      <th>ENSG00000204287</th>\n",
       "      <th>ENSG00000118785</th>\n",
       "      <th>ENSG00000230937</th>\n",
       "      <th>ENSG00000133169</th>\n",
       "      <th>ENSG00000167644</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000143333</th>\n",
       "      <th>ENSG00000196735</th>\n",
       "      <th>ENSG00000157214</th>\n",
       "      <th>ENSG00000170476</th>\n",
       "      <th>ENSG00000164294</th>\n",
       "      <th>ENSG00000079112</th>\n",
       "      <th>ENSG00000075643</th>\n",
       "      <th>ENSG00000139289</th>\n",
       "      <th>ENSG00000125850</th>\n",
       "      <th>ENSG00000125726</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENSG00000129824</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.172547</td>\n",
       "      <td>-0.154224</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>-0.186533</td>\n",
       "      <td>0.088862</td>\n",
       "      <td>-0.000511</td>\n",
       "      <td>-0.099702</td>\n",
       "      <td>0.105643</td>\n",
       "      <td>-0.177893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112090</td>\n",
       "      <td>0.128446</td>\n",
       "      <td>-0.012207</td>\n",
       "      <td>0.075844</td>\n",
       "      <td>-0.071932</td>\n",
       "      <td>-0.077886</td>\n",
       "      <td>-0.081175</td>\n",
       "      <td>-0.054768</td>\n",
       "      <td>-0.164021</td>\n",
       "      <td>0.058945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000184292</th>\n",
       "      <td>-0.172547</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.559029</td>\n",
       "      <td>-0.219109</td>\n",
       "      <td>0.685896</td>\n",
       "      <td>-0.125801</td>\n",
       "      <td>-0.104377</td>\n",
       "      <td>0.709955</td>\n",
       "      <td>-0.386670</td>\n",
       "      <td>0.654923</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267545</td>\n",
       "      <td>-0.160299</td>\n",
       "      <td>0.196770</td>\n",
       "      <td>-0.193025</td>\n",
       "      <td>0.153813</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.379531</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>0.597723</td>\n",
       "      <td>-0.268973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000163993</th>\n",
       "      <td>-0.154224</td>\n",
       "      <td>0.559029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.134828</td>\n",
       "      <td>0.543591</td>\n",
       "      <td>-0.109907</td>\n",
       "      <td>-0.106628</td>\n",
       "      <td>0.424270</td>\n",
       "      <td>-0.306688</td>\n",
       "      <td>0.506648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132693</td>\n",
       "      <td>-0.135970</td>\n",
       "      <td>0.200269</td>\n",
       "      <td>-0.166865</td>\n",
       "      <td>-0.078209</td>\n",
       "      <td>0.402660</td>\n",
       "      <td>0.353688</td>\n",
       "      <td>0.099603</td>\n",
       "      <td>0.512415</td>\n",
       "      <td>-0.305832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000122862</th>\n",
       "      <td>0.095800</td>\n",
       "      <td>-0.219109</td>\n",
       "      <td>-0.134828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.285207</td>\n",
       "      <td>0.311946</td>\n",
       "      <td>0.096311</td>\n",
       "      <td>-0.208802</td>\n",
       "      <td>-0.056677</td>\n",
       "      <td>-0.175856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>0.274049</td>\n",
       "      <td>-0.078186</td>\n",
       "      <td>0.289081</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>-0.167722</td>\n",
       "      <td>-0.062973</td>\n",
       "      <td>-0.065129</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>0.406093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000171345</th>\n",
       "      <td>-0.186533</td>\n",
       "      <td>0.685896</td>\n",
       "      <td>0.543591</td>\n",
       "      <td>-0.285207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.265346</td>\n",
       "      <td>-0.003087</td>\n",
       "      <td>0.456647</td>\n",
       "      <td>-0.249832</td>\n",
       "      <td>0.733174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258963</td>\n",
       "      <td>-0.229703</td>\n",
       "      <td>0.265831</td>\n",
       "      <td>-0.296715</td>\n",
       "      <td>0.121715</td>\n",
       "      <td>0.250129</td>\n",
       "      <td>0.363087</td>\n",
       "      <td>0.165055</td>\n",
       "      <td>0.648500</td>\n",
       "      <td>-0.311482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000079112</th>\n",
       "      <td>-0.077886</td>\n",
       "      <td>0.016765</td>\n",
       "      <td>0.402660</td>\n",
       "      <td>-0.167722</td>\n",
       "      <td>0.250129</td>\n",
       "      <td>-0.091610</td>\n",
       "      <td>-0.113976</td>\n",
       "      <td>-0.161159</td>\n",
       "      <td>-0.117890</td>\n",
       "      <td>0.239679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014074</td>\n",
       "      <td>-0.052064</td>\n",
       "      <td>0.204883</td>\n",
       "      <td>-0.046479</td>\n",
       "      <td>-0.168724</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160737</td>\n",
       "      <td>0.039392</td>\n",
       "      <td>0.241669</td>\n",
       "      <td>-0.123707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000075643</th>\n",
       "      <td>-0.081175</td>\n",
       "      <td>0.379531</td>\n",
       "      <td>0.353688</td>\n",
       "      <td>-0.062973</td>\n",
       "      <td>0.363087</td>\n",
       "      <td>-0.176922</td>\n",
       "      <td>0.145971</td>\n",
       "      <td>0.230607</td>\n",
       "      <td>-0.356555</td>\n",
       "      <td>0.473375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.355307</td>\n",
       "      <td>-0.226620</td>\n",
       "      <td>0.270314</td>\n",
       "      <td>-0.323932</td>\n",
       "      <td>0.376451</td>\n",
       "      <td>0.160737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>-0.114075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000139289</th>\n",
       "      <td>-0.054768</td>\n",
       "      <td>0.172817</td>\n",
       "      <td>0.099603</td>\n",
       "      <td>-0.065129</td>\n",
       "      <td>0.165055</td>\n",
       "      <td>-0.148803</td>\n",
       "      <td>0.215056</td>\n",
       "      <td>0.131172</td>\n",
       "      <td>-0.185868</td>\n",
       "      <td>0.342359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.413273</td>\n",
       "      <td>-0.209523</td>\n",
       "      <td>0.309539</td>\n",
       "      <td>-0.457291</td>\n",
       "      <td>0.530755</td>\n",
       "      <td>0.039392</td>\n",
       "      <td>0.411549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.037749</td>\n",
       "      <td>-0.087807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000125850</th>\n",
       "      <td>-0.164021</td>\n",
       "      <td>0.597723</td>\n",
       "      <td>0.512415</td>\n",
       "      <td>-0.392380</td>\n",
       "      <td>0.648500</td>\n",
       "      <td>-0.208162</td>\n",
       "      <td>-0.135227</td>\n",
       "      <td>0.426460</td>\n",
       "      <td>-0.109122</td>\n",
       "      <td>0.441673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040930</td>\n",
       "      <td>-0.178070</td>\n",
       "      <td>0.112117</td>\n",
       "      <td>-0.182985</td>\n",
       "      <td>-0.131150</td>\n",
       "      <td>0.241669</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>-0.037749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.398243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENSG00000125726</th>\n",
       "      <td>0.058945</td>\n",
       "      <td>-0.268973</td>\n",
       "      <td>-0.305832</td>\n",
       "      <td>0.406093</td>\n",
       "      <td>-0.311482</td>\n",
       "      <td>0.378510</td>\n",
       "      <td>0.075520</td>\n",
       "      <td>-0.195625</td>\n",
       "      <td>-0.112411</td>\n",
       "      <td>-0.163908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>0.349399</td>\n",
       "      <td>-0.081285</td>\n",
       "      <td>0.321460</td>\n",
       "      <td>-0.014188</td>\n",
       "      <td>-0.123707</td>\n",
       "      <td>-0.114075</td>\n",
       "      <td>-0.087807</td>\n",
       "      <td>-0.398243</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ENSG00000129824  ENSG00000184292  ENSG00000163993  \\\n",
       "ENSG00000129824         1.000000        -0.172547        -0.154224   \n",
       "ENSG00000184292        -0.172547         1.000000         0.559029   \n",
       "ENSG00000163993        -0.154224         0.559029         1.000000   \n",
       "ENSG00000122862         0.095800        -0.219109        -0.134828   \n",
       "ENSG00000171345        -0.186533         0.685896         0.543591   \n",
       "...                          ...              ...              ...   \n",
       "ENSG00000079112        -0.077886         0.016765         0.402660   \n",
       "ENSG00000075643        -0.081175         0.379531         0.353688   \n",
       "ENSG00000139289        -0.054768         0.172817         0.099603   \n",
       "ENSG00000125850        -0.164021         0.597723         0.512415   \n",
       "ENSG00000125726         0.058945        -0.268973        -0.305832   \n",
       "\n",
       "                 ENSG00000122862  ENSG00000171345  ENSG00000204287  \\\n",
       "ENSG00000129824         0.095800        -0.186533         0.088862   \n",
       "ENSG00000184292        -0.219109         0.685896        -0.125801   \n",
       "ENSG00000163993        -0.134828         0.543591        -0.109907   \n",
       "ENSG00000122862         1.000000        -0.285207         0.311946   \n",
       "ENSG00000171345        -0.285207         1.000000        -0.265346   \n",
       "...                          ...              ...              ...   \n",
       "ENSG00000079112        -0.167722         0.250129        -0.091610   \n",
       "ENSG00000075643        -0.062973         0.363087        -0.176922   \n",
       "ENSG00000139289        -0.065129         0.165055        -0.148803   \n",
       "ENSG00000125850        -0.392380         0.648500        -0.208162   \n",
       "ENSG00000125726         0.406093        -0.311482         0.378510   \n",
       "\n",
       "                 ENSG00000118785  ENSG00000230937  ENSG00000133169  \\\n",
       "ENSG00000129824        -0.000511        -0.099702         0.105643   \n",
       "ENSG00000184292        -0.104377         0.709955        -0.386670   \n",
       "ENSG00000163993        -0.106628         0.424270        -0.306688   \n",
       "ENSG00000122862         0.096311        -0.208802        -0.056677   \n",
       "ENSG00000171345        -0.003087         0.456647        -0.249832   \n",
       "...                          ...              ...              ...   \n",
       "ENSG00000079112        -0.113976        -0.161159        -0.117890   \n",
       "ENSG00000075643         0.145971         0.230607        -0.356555   \n",
       "ENSG00000139289         0.215056         0.131172        -0.185868   \n",
       "ENSG00000125850        -0.135227         0.426460        -0.109122   \n",
       "ENSG00000125726         0.075520        -0.195625        -0.112411   \n",
       "\n",
       "                 ENSG00000167644  ...  ENSG00000143333  ENSG00000196735  \\\n",
       "ENSG00000129824        -0.177893  ...         0.112090         0.128446   \n",
       "ENSG00000184292         0.654923  ...        -0.267545        -0.160299   \n",
       "ENSG00000163993         0.506648  ...        -0.132693        -0.135970   \n",
       "ENSG00000122862        -0.175856  ...        -0.028097         0.274049   \n",
       "ENSG00000171345         0.733174  ...        -0.258963        -0.229703   \n",
       "...                          ...  ...              ...              ...   \n",
       "ENSG00000079112         0.239679  ...        -0.014074        -0.052064   \n",
       "ENSG00000075643         0.473375  ...        -0.355307        -0.226620   \n",
       "ENSG00000139289         0.342359  ...        -0.413273        -0.209523   \n",
       "ENSG00000125850         0.441673  ...        -0.040930        -0.178070   \n",
       "ENSG00000125726        -0.163908  ...         0.062644         0.349399   \n",
       "\n",
       "                 ENSG00000157214  ENSG00000170476  ENSG00000164294  \\\n",
       "ENSG00000129824        -0.012207         0.075844        -0.071932   \n",
       "ENSG00000184292         0.196770        -0.193025         0.153813   \n",
       "ENSG00000163993         0.200269        -0.166865        -0.078209   \n",
       "ENSG00000122862        -0.078186         0.289081         0.020553   \n",
       "ENSG00000171345         0.265831        -0.296715         0.121715   \n",
       "...                          ...              ...              ...   \n",
       "ENSG00000079112         0.204883        -0.046479        -0.168724   \n",
       "ENSG00000075643         0.270314        -0.323932         0.376451   \n",
       "ENSG00000139289         0.309539        -0.457291         0.530755   \n",
       "ENSG00000125850         0.112117        -0.182985        -0.131150   \n",
       "ENSG00000125726        -0.081285         0.321460        -0.014188   \n",
       "\n",
       "                 ENSG00000079112  ENSG00000075643  ENSG00000139289  \\\n",
       "ENSG00000129824        -0.077886        -0.081175        -0.054768   \n",
       "ENSG00000184292         0.016765         0.379531         0.172817   \n",
       "ENSG00000163993         0.402660         0.353688         0.099603   \n",
       "ENSG00000122862        -0.167722        -0.062973        -0.065129   \n",
       "ENSG00000171345         0.250129         0.363087         0.165055   \n",
       "...                          ...              ...              ...   \n",
       "ENSG00000079112         1.000000         0.160737         0.039392   \n",
       "ENSG00000075643         0.160737         1.000000         0.411549   \n",
       "ENSG00000139289         0.039392         0.411549         1.000000   \n",
       "ENSG00000125850         0.241669         0.202509        -0.037749   \n",
       "ENSG00000125726        -0.123707        -0.114075        -0.087807   \n",
       "\n",
       "                 ENSG00000125850  ENSG00000125726  \n",
       "ENSG00000129824        -0.164021         0.058945  \n",
       "ENSG00000184292         0.597723        -0.268973  \n",
       "ENSG00000163993         0.512415        -0.305832  \n",
       "ENSG00000122862        -0.392380         0.406093  \n",
       "ENSG00000171345         0.648500        -0.311482  \n",
       "...                          ...              ...  \n",
       "ENSG00000079112         0.241669        -0.123707  \n",
       "ENSG00000075643         0.202509        -0.114075  \n",
       "ENSG00000139289        -0.037749        -0.087807  \n",
       "ENSG00000125850         1.000000        -0.398243  \n",
       "ENSG00000125726        -0.398243         1.000000  \n",
       "\n",
       "[500 rows x 500 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_500_cols = X_train.iloc[:, top_500_coef_var]\n",
    "corr = top_500_cols.corr()\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns are corralated. Due to that fact we have to use regression method that can deal with correlated variables. Lasso and ridge regression are the methods that could help. To know which one of them will perform better we will use Elastic Net model. Elastic net is the compromise between Lasso and ridge regression. Contribution of these two models can be balanced using \"l1_ratio\". I will also use cross validation to obtain better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_score(l1_ratio):\n",
    "    scores = (-1)*cross_val_score(ElasticNet(l1_ratio=l1_ratio, tol=0.001, max_iter=1000, random_state=1), X_train, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szyka\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.849e-01, tolerance: 9.435e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\szyka\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.839e-01, tolerance: 9.143e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\szyka\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e-01, tolerance: 1.053e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\szyka\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.927e-01, tolerance: 9.664e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\szyka\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.277e-01, tolerance: 1.039e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for i in range(0, 11, 1):\n",
    "    results[i]=get_score(i*0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.014453098533445102,\n",
       " 1: 0.01491560530903736,\n",
       " 2: 0.018723889285898708,\n",
       " 3: 0.019127634382155007,\n",
       " 4: 0.019127634382155007,\n",
       " 5: 0.019127634382155007,\n",
       " 6: 0.019127634382155007,\n",
       " 7: 0.019127634382155007,\n",
       " 8: 0.019127634382155007,\n",
       " 9: 0.019127634382155007,\n",
       " 10: 0.019127634382155007}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for several models there is a warning. Let's try to increase number of iteration to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_score_iter(l1_ratio):\\n    scores = (-1)*cross_val_score(ElasticNet(l1_ratio=l1_ratio, max_iter=10000, random_state=1), X_train, y, cv=5, scoring='neg_mean_squared_error')\\n    return scores.mean()\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_score_iter(l1_ratio):\n",
    "    scores = (-1)*cross_val_score(ElasticNet(l1_ratio=l1_ratio, max_iter=10000, random_state=1), X_train, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results_iter = \\nfor i in range(0, 11, 1):\\n    results[i]=get_score_iter(i*0.1)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_iter = \n",
    "for i in range(0, 11, 1):\n",
    "    results[i]=get_score_iter(i*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After over 10 minutes we still have some warnings. Maybe best result model (l1_ratio=0) is not giving any problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szyka\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.725e-01, tolerance: 1.229e-02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ElasticNet(l1_ratio=0, max_iter=10000, random_state=1, tol=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ElasticNet</label><div class=\"sk-toggleable__content\"><pre>ElasticNet(l1_ratio=0, max_iter=10000, random_state=1, tol=0.001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ElasticNet(l1_ratio=0, max_iter=10000, random_state=1, tol=0.001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en = ElasticNet(l1_ratio=0, tol=0.001, max_iter=10000, random_state=1)\n",
    "model_en.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we obtain best results when \"l1_ratio\" is equal to 0. It means that best model for our data set is ridge regression model. In my opinion it is because there is enormously more variables than observations. That is the situation when ridge regression can perform better than Lasso. But in the warning we have information that we can try RidgeCV that is fitted more efficiently. Let's try that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RidgeCV(cv=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV(cv=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RidgeCV(cv=5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_r = RidgeCV(cv=5)\n",
    "model_r.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is no warnings. Now we will invastigate mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97473485, 0.90293189, 0.93184013, 0.95569422, 0.8691048 ,\n",
       "       0.77426137, 0.9158673 , 0.81362996, 0.88068154, 0.85935401,\n",
       "       1.05373807, 0.87857979, 0.82332545, 0.63403935, 0.95539089,\n",
       "       1.03634037, 0.8275145 , 0.94098437, 0.6933974 , 0.77360648,\n",
       "       0.65057646, 0.88651433, 0.84469309, 0.74017961, 0.88764595,\n",
       "       0.90325946, 0.88265411, 0.65470248, 0.97456774, 0.68178456,\n",
       "       0.80793497, 0.94990361, 0.89141129, 0.72763408, 0.89959846,\n",
       "       0.69222128, 0.91504013, 0.88319335, 0.52354676, 0.65937964,\n",
       "       0.86887114, 0.7611533 , 0.90206938, 1.0060718 , 0.86870953,\n",
       "       0.99035289, 0.73197086, 0.66077662, 0.95772352, 0.87399934,\n",
       "       0.93292172, 0.76721109, 0.96053407, 0.93122423, 0.82426678,\n",
       "       0.95106247, 0.90435489, 0.97323577, 0.82089764, 0.91927147,\n",
       "       0.83303758, 0.83484209, 0.9738336 , 0.97913081, 0.93315069,\n",
       "       0.83975975, 0.96969512, 0.75047593, 0.58279065, 0.90289292,\n",
       "       0.91631355, 0.38390333, 0.91039448, 0.95269582, 0.94085103,\n",
       "       0.94038073, 0.85150156, 0.86368514, 0.82387203, 0.91312632,\n",
       "       0.96150414, 0.70300001, 0.97875076, 0.6997689 , 0.92813941,\n",
       "       0.93902277, 0.87175768, 0.94488961, 0.76158695, 0.78328267,\n",
       "       0.84893764, 0.85251543, 0.87961426, 0.85419755, 0.91405136,\n",
       "       0.48111573, 0.92231069, 0.85293777, 0.87253667, 0.80715989,\n",
       "       1.02366973, 0.84116218, 0.6434808 , 0.92385611, 0.94451993,\n",
       "       0.89510006, 1.01320404, 0.70586407, 0.94005275, 0.87783243,\n",
       "       0.88041403, 0.69344405, 0.68855746, 1.01645547, 0.63460052,\n",
       "       0.99155739, 0.80253461, 0.93666242, 0.63472335, 0.95943696,\n",
       "       1.03695677, 0.91782109, 0.6880214 , 0.88749702, 0.94542342,\n",
       "       0.81971542, 0.7660663 , 0.93747494, 0.76067298, 0.93075891,\n",
       "       0.94274365, 0.96585148, 0.90383677, 0.74716796, 0.91065589,\n",
       "       0.8047365 , 0.91488405, 0.84992931, 0.7745191 , 0.78412139,\n",
       "       0.72681899, 0.98149611, 0.8036426 , 0.46180442, 0.81141519,\n",
       "       0.79913732, 0.8855843 , 0.875266  , 0.84600952, 0.89461115,\n",
       "       0.61214072, 0.90602458, 1.0245972 , 0.8758505 , 0.77141538,\n",
       "       0.81481076, 0.84554297, 0.80826122, 0.89885015, 0.92959968,\n",
       "       0.76856201, 0.86024389, 0.6327953 , 0.89726102, 0.88150217,\n",
       "       0.73029686, 0.84148259, 0.87815862, 0.88058263, 1.06086754,\n",
       "       0.97713755, 0.67824641, 0.72964894, 0.98450257, 0.87393657,\n",
       "       0.72809468, 0.69195145, 0.99698667, 0.76485073, 0.87871772,\n",
       "       0.77200461, 0.87170657, 0.84461873, 0.98521857, 0.81647392,\n",
       "       0.83133838, 0.87245255, 0.82564105, 0.78044387, 0.78220832,\n",
       "       0.62708282, 0.97900307, 0.94183306, 0.75920309, 0.76415678,\n",
       "       0.71258055, 0.44254699, 0.87766655, 0.89933347, 0.98452733,\n",
       "       0.97521728, 0.76769327, 0.8049155 , 0.88032567, 0.86285615,\n",
       "       0.7334998 , 0.69232728, 0.80268223, 0.94829485, 0.6854731 ,\n",
       "       0.87556853, 0.91447258, 0.94144571, 0.87853206, 0.88921467,\n",
       "       0.8050981 , 0.80621223, 0.85709474, 1.00348282, 0.8089615 ,\n",
       "       0.8332954 , 0.92451343, 0.88361076, 0.86199673, 0.44949812,\n",
       "       0.88419681, 0.98350623, 0.93645172, 0.96351388, 0.87205142,\n",
       "       0.93996717, 0.89393291, 0.85295874, 0.58315465, 0.82705102,\n",
       "       0.94915844, 0.73804745, 0.94590254, 0.88020425, 0.8218041 ,\n",
       "       0.81805124, 0.74507257, 0.74842686, 0.93036521, 0.72227717,\n",
       "       0.97122611, 0.85794976, 0.79095116, 0.87072307, 0.93943756,\n",
       "       0.84066435, 0.91236831, 0.94556874, 0.90988922, 0.92267483,\n",
       "       0.86715245, 0.87352248, 0.89299798, 0.96772055, 0.81435115,\n",
       "       0.87927028, 0.86424374, 0.66291152, 0.9313566 , 0.93559402,\n",
       "       1.03797564, 0.71882641, 0.66817728, 0.73768932, 0.96511446,\n",
       "       0.99195265, 0.91052464, 0.91070368, 0.90427164, 0.97583405,\n",
       "       0.93084037])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check how Random forest model will perform on our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000129824</th>\n",
       "      <th>ENSG00000184292</th>\n",
       "      <th>ENSG00000163993</th>\n",
       "      <th>ENSG00000122862</th>\n",
       "      <th>ENSG00000171345</th>\n",
       "      <th>ENSG00000204287</th>\n",
       "      <th>ENSG00000118785</th>\n",
       "      <th>ENSG00000230937</th>\n",
       "      <th>ENSG00000133169</th>\n",
       "      <th>ENSG00000167644</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000143333</th>\n",
       "      <th>ENSG00000196735</th>\n",
       "      <th>ENSG00000157214</th>\n",
       "      <th>ENSG00000170476</th>\n",
       "      <th>ENSG00000164294</th>\n",
       "      <th>ENSG00000079112</th>\n",
       "      <th>ENSG00000075643</th>\n",
       "      <th>ENSG00000139289</th>\n",
       "      <th>ENSG00000125850</th>\n",
       "      <th>ENSG00000125726</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.182198</td>\n",
       "      <td>4.033507</td>\n",
       "      <td>12.141730</td>\n",
       "      <td>2.870359</td>\n",
       "      <td>12.020272</td>\n",
       "      <td>2.853857</td>\n",
       "      <td>2.976589</td>\n",
       "      <td>2.975880</td>\n",
       "      <td>3.077572</td>\n",
       "      <td>9.829330</td>\n",
       "      <td>...</td>\n",
       "      <td>3.473683</td>\n",
       "      <td>3.088935</td>\n",
       "      <td>3.735464</td>\n",
       "      <td>3.163082</td>\n",
       "      <td>4.089234</td>\n",
       "      <td>9.148311</td>\n",
       "      <td>7.670131</td>\n",
       "      <td>8.681529</td>\n",
       "      <td>6.370924</td>\n",
       "      <td>4.016640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.896301</td>\n",
       "      <td>11.415213</td>\n",
       "      <td>12.823129</td>\n",
       "      <td>2.904678</td>\n",
       "      <td>12.778011</td>\n",
       "      <td>10.726161</td>\n",
       "      <td>2.925108</td>\n",
       "      <td>10.208873</td>\n",
       "      <td>4.193578</td>\n",
       "      <td>9.650949</td>\n",
       "      <td>...</td>\n",
       "      <td>3.393908</td>\n",
       "      <td>3.125673</td>\n",
       "      <td>6.895485</td>\n",
       "      <td>3.991203</td>\n",
       "      <td>5.920513</td>\n",
       "      <td>2.879645</td>\n",
       "      <td>6.931864</td>\n",
       "      <td>6.373925</td>\n",
       "      <td>8.019916</td>\n",
       "      <td>3.965501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.737682</td>\n",
       "      <td>10.666418</td>\n",
       "      <td>3.635745</td>\n",
       "      <td>2.916668</td>\n",
       "      <td>12.108477</td>\n",
       "      <td>3.551805</td>\n",
       "      <td>2.882070</td>\n",
       "      <td>3.084958</td>\n",
       "      <td>3.352948</td>\n",
       "      <td>11.102829</td>\n",
       "      <td>...</td>\n",
       "      <td>3.291920</td>\n",
       "      <td>3.003933</td>\n",
       "      <td>8.325705</td>\n",
       "      <td>3.336496</td>\n",
       "      <td>6.548224</td>\n",
       "      <td>3.004829</td>\n",
       "      <td>3.715563</td>\n",
       "      <td>10.561546</td>\n",
       "      <td>7.056477</td>\n",
       "      <td>3.618266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.512189</td>\n",
       "      <td>10.590466</td>\n",
       "      <td>11.987303</td>\n",
       "      <td>2.938763</td>\n",
       "      <td>11.720451</td>\n",
       "      <td>3.162247</td>\n",
       "      <td>3.016976</td>\n",
       "      <td>7.606048</td>\n",
       "      <td>3.424096</td>\n",
       "      <td>3.062215</td>\n",
       "      <td>...</td>\n",
       "      <td>4.275209</td>\n",
       "      <td>3.057965</td>\n",
       "      <td>2.915628</td>\n",
       "      <td>5.240844</td>\n",
       "      <td>2.677779</td>\n",
       "      <td>3.137427</td>\n",
       "      <td>4.397014</td>\n",
       "      <td>3.442794</td>\n",
       "      <td>4.555335</td>\n",
       "      <td>4.018431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.961343</td>\n",
       "      <td>9.888883</td>\n",
       "      <td>4.718643</td>\n",
       "      <td>2.858609</td>\n",
       "      <td>12.996339</td>\n",
       "      <td>2.814782</td>\n",
       "      <td>2.880148</td>\n",
       "      <td>3.130928</td>\n",
       "      <td>3.237267</td>\n",
       "      <td>2.927342</td>\n",
       "      <td>...</td>\n",
       "      <td>3.394207</td>\n",
       "      <td>2.964157</td>\n",
       "      <td>2.651286</td>\n",
       "      <td>4.214614</td>\n",
       "      <td>6.468536</td>\n",
       "      <td>2.888658</td>\n",
       "      <td>5.218814</td>\n",
       "      <td>3.853191</td>\n",
       "      <td>5.471416</td>\n",
       "      <td>4.065496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>3.786291</td>\n",
       "      <td>11.211629</td>\n",
       "      <td>12.969220</td>\n",
       "      <td>2.956050</td>\n",
       "      <td>12.637708</td>\n",
       "      <td>3.048539</td>\n",
       "      <td>2.969986</td>\n",
       "      <td>3.135039</td>\n",
       "      <td>3.311895</td>\n",
       "      <td>10.415566</td>\n",
       "      <td>...</td>\n",
       "      <td>3.734044</td>\n",
       "      <td>2.937078</td>\n",
       "      <td>8.533649</td>\n",
       "      <td>3.359658</td>\n",
       "      <td>6.021126</td>\n",
       "      <td>9.082506</td>\n",
       "      <td>8.149750</td>\n",
       "      <td>10.410169</td>\n",
       "      <td>5.806246</td>\n",
       "      <td>4.016926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>4.018057</td>\n",
       "      <td>11.941617</td>\n",
       "      <td>12.676235</td>\n",
       "      <td>8.757200</td>\n",
       "      <td>11.089748</td>\n",
       "      <td>2.941417</td>\n",
       "      <td>3.095832</td>\n",
       "      <td>11.566505</td>\n",
       "      <td>3.333017</td>\n",
       "      <td>9.138738</td>\n",
       "      <td>...</td>\n",
       "      <td>3.656884</td>\n",
       "      <td>3.104766</td>\n",
       "      <td>3.699962</td>\n",
       "      <td>3.629262</td>\n",
       "      <td>6.978750</td>\n",
       "      <td>2.893266</td>\n",
       "      <td>9.378995</td>\n",
       "      <td>7.161075</td>\n",
       "      <td>6.269668</td>\n",
       "      <td>4.122071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>2.932608</td>\n",
       "      <td>3.003441</td>\n",
       "      <td>9.758864</td>\n",
       "      <td>2.975189</td>\n",
       "      <td>11.360486</td>\n",
       "      <td>3.024615</td>\n",
       "      <td>3.079584</td>\n",
       "      <td>3.034243</td>\n",
       "      <td>3.064740</td>\n",
       "      <td>9.816923</td>\n",
       "      <td>...</td>\n",
       "      <td>5.943107</td>\n",
       "      <td>3.063674</td>\n",
       "      <td>6.729988</td>\n",
       "      <td>3.159595</td>\n",
       "      <td>4.729276</td>\n",
       "      <td>9.484206</td>\n",
       "      <td>10.145304</td>\n",
       "      <td>5.665610</td>\n",
       "      <td>6.153742</td>\n",
       "      <td>4.090919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>12.119873</td>\n",
       "      <td>11.719849</td>\n",
       "      <td>3.853993</td>\n",
       "      <td>3.151453</td>\n",
       "      <td>12.206070</td>\n",
       "      <td>5.484684</td>\n",
       "      <td>2.936075</td>\n",
       "      <td>7.094722</td>\n",
       "      <td>3.358969</td>\n",
       "      <td>9.965068</td>\n",
       "      <td>...</td>\n",
       "      <td>3.374608</td>\n",
       "      <td>3.074905</td>\n",
       "      <td>6.693722</td>\n",
       "      <td>3.259892</td>\n",
       "      <td>6.199547</td>\n",
       "      <td>3.043489</td>\n",
       "      <td>9.808196</td>\n",
       "      <td>6.602170</td>\n",
       "      <td>7.101028</td>\n",
       "      <td>4.022826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>12.971574</td>\n",
       "      <td>2.948015</td>\n",
       "      <td>10.472113</td>\n",
       "      <td>6.944167</td>\n",
       "      <td>3.277413</td>\n",
       "      <td>5.398311</td>\n",
       "      <td>3.193826</td>\n",
       "      <td>2.778898</td>\n",
       "      <td>9.376422</td>\n",
       "      <td>2.731067</td>\n",
       "      <td>...</td>\n",
       "      <td>5.105826</td>\n",
       "      <td>2.914192</td>\n",
       "      <td>8.574264</td>\n",
       "      <td>3.902886</td>\n",
       "      <td>5.405127</td>\n",
       "      <td>2.937394</td>\n",
       "      <td>4.084982</td>\n",
       "      <td>4.411564</td>\n",
       "      <td>3.124562</td>\n",
       "      <td>7.699640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ENSG00000129824  ENSG00000184292  ENSG00000163993  ENSG00000122862  \\\n",
       "0           3.182198         4.033507        12.141730         2.870359   \n",
       "1           2.896301        11.415213        12.823129         2.904678   \n",
       "2           2.737682        10.666418         3.635745         2.916668   \n",
       "3           3.512189        10.590466        11.987303         2.938763   \n",
       "4           2.961343         9.888883         4.718643         2.858609   \n",
       "..               ...              ...              ...              ...   \n",
       "639         3.786291        11.211629        12.969220         2.956050   \n",
       "640         4.018057        11.941617        12.676235         8.757200   \n",
       "641         2.932608         3.003441         9.758864         2.975189   \n",
       "642        12.119873        11.719849         3.853993         3.151453   \n",
       "643        12.971574         2.948015        10.472113         6.944167   \n",
       "\n",
       "     ENSG00000171345  ENSG00000204287  ENSG00000118785  ENSG00000230937  \\\n",
       "0          12.020272         2.853857         2.976589         2.975880   \n",
       "1          12.778011        10.726161         2.925108        10.208873   \n",
       "2          12.108477         3.551805         2.882070         3.084958   \n",
       "3          11.720451         3.162247         3.016976         7.606048   \n",
       "4          12.996339         2.814782         2.880148         3.130928   \n",
       "..               ...              ...              ...              ...   \n",
       "639        12.637708         3.048539         2.969986         3.135039   \n",
       "640        11.089748         2.941417         3.095832        11.566505   \n",
       "641        11.360486         3.024615         3.079584         3.034243   \n",
       "642        12.206070         5.484684         2.936075         7.094722   \n",
       "643         3.277413         5.398311         3.193826         2.778898   \n",
       "\n",
       "     ENSG00000133169  ENSG00000167644  ...  ENSG00000143333  ENSG00000196735  \\\n",
       "0           3.077572         9.829330  ...         3.473683         3.088935   \n",
       "1           4.193578         9.650949  ...         3.393908         3.125673   \n",
       "2           3.352948        11.102829  ...         3.291920         3.003933   \n",
       "3           3.424096         3.062215  ...         4.275209         3.057965   \n",
       "4           3.237267         2.927342  ...         3.394207         2.964157   \n",
       "..               ...              ...  ...              ...              ...   \n",
       "639         3.311895        10.415566  ...         3.734044         2.937078   \n",
       "640         3.333017         9.138738  ...         3.656884         3.104766   \n",
       "641         3.064740         9.816923  ...         5.943107         3.063674   \n",
       "642         3.358969         9.965068  ...         3.374608         3.074905   \n",
       "643         9.376422         2.731067  ...         5.105826         2.914192   \n",
       "\n",
       "     ENSG00000157214  ENSG00000170476  ENSG00000164294  ENSG00000079112  \\\n",
       "0           3.735464         3.163082         4.089234         9.148311   \n",
       "1           6.895485         3.991203         5.920513         2.879645   \n",
       "2           8.325705         3.336496         6.548224         3.004829   \n",
       "3           2.915628         5.240844         2.677779         3.137427   \n",
       "4           2.651286         4.214614         6.468536         2.888658   \n",
       "..               ...              ...              ...              ...   \n",
       "639         8.533649         3.359658         6.021126         9.082506   \n",
       "640         3.699962         3.629262         6.978750         2.893266   \n",
       "641         6.729988         3.159595         4.729276         9.484206   \n",
       "642         6.693722         3.259892         6.199547         3.043489   \n",
       "643         8.574264         3.902886         5.405127         2.937394   \n",
       "\n",
       "     ENSG00000075643  ENSG00000139289  ENSG00000125850  ENSG00000125726  \n",
       "0           7.670131         8.681529         6.370924         4.016640  \n",
       "1           6.931864         6.373925         8.019916         3.965501  \n",
       "2           3.715563        10.561546         7.056477         3.618266  \n",
       "3           4.397014         3.442794         4.555335         4.018431  \n",
       "4           5.218814         3.853191         5.471416         4.065496  \n",
       "..               ...              ...              ...              ...  \n",
       "639         8.149750        10.410169         5.806246         4.016926  \n",
       "640         9.378995         7.161075         6.269668         4.122071  \n",
       "641        10.145304         5.665610         6.153742         4.090919  \n",
       "642         9.808196         6.602170         7.101028         4.022826  \n",
       "643         4.084982         4.411564         3.124562         7.699640  \n",
       "\n",
       "[644 rows x 500 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_rfr = top_500_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_rfr(n_estimators):\n",
    "    scores = (-1)*cross_val_score(RandomForestRegressor(n_estimators=n_estimators), X_train_rfr, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0.013338986333989694,\n",
       " 20: 0.013066390455454126,\n",
       " 30: 0.013216604950045025,\n",
       " 40: 0.012748181938118386,\n",
       " 50: 0.012598471800869416,\n",
       " 60: 0.012890534464041318,\n",
       " 70: 0.012646492288545119,\n",
       " 80: 0.012738910132888175,\n",
       " 90: 0.012433046736804234,\n",
       " 100: 0.012722207644401923}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rfr = {}\n",
    "for i in range(10, 110, 10):\n",
    "    results_rfr[i] = get_score_rfr(i)\n",
    "results_rfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best model is for n_estimators = 90"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "531782c982964c65c938113cbb69c0a534a2104b0692198f1e69c6c5005775c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
